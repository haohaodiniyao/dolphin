		      所属集群
server1  cluster1 
server4  cluster1
server5  cluster2
server6  cluster2

HDFS Federation(联邦)
验证通过
http://www.cnblogs.com/gaogf/p/3982292.html

###hadoop-2.8.1.tar.gz安装
安装博客http://blog.csdn.net/zzpzheng/article/details/73614526
#####SSH免密码登录
http://blog.csdn.net/wanglei_storage/article/details/52853034
生成秘钥对
[root@server2 ~]# ssh-keygen -t rsa
秘钥下发
[root@server2 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.157.129
####配置文件详解
http://www.linuxidc.com/Linux/2012-07/66286.htm
http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-resourcemanager-nodemanager/
####HDFS HA QJM 高可用
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html
####resource manager ha######
http://debugo.com/yarn-rm-ha/
http://debugo.com/yarn-rm-ha/#more-960

#####hadoop HA+Federation（高可用联邦）模式搭建指南
http://www.jianshu.com/p/ccee07a31ca9
#####hadoop ha federation#####
http://www.cnblogs.com/meiyuanbao/p/3545929.html
http://blog.csdn.net/lfq1532632051/article/details/42212245
http://www.linuxidc.com/Linux/2014-10/107933.htm



####启动rm
/usr/local/src/hadoop-2.8.1/sbin/yarn-daemon.sh start resourcemanager
[root@server1 bin]# /usr/local/src/hadoop-2.8.1/bin/yarn rmadmin -getServiceState rm1
active
web page
http://192.168.157.132:8188/cluster/cluster

实战
http://blog.csdn.net/fengye_yulu/article/details/69490575


/usr/local/src/hadoop-2.8.1/sbin/hadoop-daemon.sh stop journalnode

首先执行
./hadoop-daemon.sh start journalnode
./hdfs zkfc -formatZK

然后在格式化
./hadoop namenode -format
#####hadoop2.8 HA配置
http://blog.csdn.net/darkdragonking/article/details/72781782
###core-site.xml
<property>
	<name>hadoop.tmp.dir</name>
	<value>/usr/local/src/hadoop/tmp</value>
</property>
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://server1:9000</value>
</property>
###hadoop-env.sh
export JAVA_HOME=/usr/local/src/jdk1.7.0_80
###hdfs-site.xml
    <property>
        <name>dfs.name.dir</name>
        <value>/usr/local/src/hadoop/dfs/name</value>
    </property>
    <property>
        <name>dfs.data.dir</name>
        <value>/usr/local/src/hadoop/dfs/data</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
###mapred-site.xml    
    <property>
        <name>mapred.job.tracker</name>
        <value>server1:49001</value>
    </property>
    <property>
        <name>mapred.local.dir</name>
        <value>/usr/local/src/hadoop/var</value>
    </property>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
###vi slaves  (其他机器没有slaves)    
server2
server3
###yarn-site.xml
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>server1</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
###启动###
./start-all.sh    
###访问
http://192.168.157.128:50070
http://192.168.157.128:8088/cluster
###slave机器状态
http://192.168.157.129:8042/node  




####
创建文件夹
./hadoop fs -mkdir -p /usr/local/src/input
上传文件
./hadoop fs -put /usr/local/src/sparkWordCount /usr/local/src/input

####多文件结果合并
./hdfs dfs -getmerge hdfs://192.168.157.128:9000/sogou/output1 /usr/local/src/result1





#####yarn#####
yarn资源调度和隔离(经典)
http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-memory-cpu-scheduling/


#####Hadoop 2.0 NameNode HA和Federation实践##### 
http://www.sizeofvoid.net/hadoop-2-0-namenode-ha-federation-practice-zh/




1- 启动journalnode
/usr/local/src/hadoop-2.8.1/sbin/hadoop-daemon.sh start journalnode
2- 格式化(HA主节点)
/usr/local/src/hadoop-2.8.1/bin/hdfs zkfc -formatZK
----- federation 共用clusterId c1
/usr/local/src/hadoop-2.8.1/bin/hdfs namenode -format -clusterId c1
3- 启动(HA主节点)
/usr/local/src/hadoop-2.8.1/sbin/hadoop-daemon.sh start namenode
4- standby节点同步元数据
/usr/local/src/hadoop-2.8.1/bin/hdfs namenode -bootstrapStandby
5- 启动standby namenode
/usr/local/src/hadoop-2.8.1/sbin/hadoop-daemon.sh start namenode
-----所有节点启动zkfc
/usr/local/src/hadoop-2.8.1/sbin/hadoop-daemon.sh start zkfc
/usr/local/src/hadoop-2.8.1/sbin/hadoop-daemon.sh stop zkfc

6- 启动所有 datanode
/usr/local/src/hadoop-2.8.1/sbin/hadoop-daemon.sh start datanode
7- 启动yarn
/usr/local/src/hadoop-2.8.1/sbin/start-yarn.sh
8-